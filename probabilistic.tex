\section{Probabilistic modeling}

\textbf{Bayes optimal predictor}: $h^*(x) = \E[Y \mid X = x]$, \red{unattainable in pr.}
Can try to estimate conditional distr.\ $\hat{P}(Y\mid X)$.

\emph{Parametric estimation}: Have $\hat{P}(Y \mid X, \theta)$, 

MLE: $\theta^* = \argmax_\theta \hat{P}(Y \mid X, \theta) = \argmin_\theta -\sum_{i=1}^n \log\hat{P}(y_i \mid x_i, \theta)$

E.g. Gauss. noise, lin. reg.: $y_i \sim \mathcal{N}(w^T x, \sigma^2)$

$\argmin_\theta = \frac{n}{2} \log(2 \pi \sigma^2) \sum_{i=1}^n \frac{(y_i - w^T x_i)^2}{2 \sigma^2}$,

MLE equiv. to LSQ est.. In general, MLE with gauss. noise with const. var. \green{equiv.} to LSQ sol.

\textbf{Bias Variance Tradeoff}: Prediction error = $\text{Bias}^2$ + Variance + Noise. Bias: Risk of best model compared to minimal risk given $P(X,Y)$. Variance: Risk due to estimating model from limited data. Noise: Risk by optimal model. \green{Trade bias and variance via model selection / regularization}

\textbf{MAP est.}: $\argmax_w P(w | x_{1:n}, y_{1:n}) = \argmax_w \frac{P(w) P(y_{1:n} | x_{1:n}, w )}{P(y_{1:n} | x_{1:n})} = \argmin_w - \log P(w) - \log P(y_{1:n} | x_{1:n}, w )$ For Gaussian noise and Gaussian prior: MAP = Ridge regression. Regularized estimation can often be understood as MAP inference:  $\argmin_w \sum_{i=1}^n \ell(w^T x_i; x_i, y_i) + C(w) = \argmax_w \prod_i P(y_i | x_i, w) P(w) = \argmax_w P(w|D)$ where $C(w) = - \log P(w)$ and $\ell(w^T x_i; x_i, y_i) = - \log P(y_i | x_i, w)$.

1. Choose likelihood function $\rightarrow$ loss function

2. Choose prior $\rightarrow$ regularizer

3. Optimize for MAP parameters, choose hyperparameters through cross-validation

4. Make predictions via B. Decision Theory

\textbf{Bayes optimal classifier}: $h^*(x) = \argmax_y P(Y = y \mid X = x)$

\textbf{Logistic regression}: $P(Y=y|x) = \frac{1}{1+\exp(-y w^T x)}$. Replaces Gauss. noise assumption with Ber. noise: $P(y|x,w)=\text{Ber}(y;\sigma(w^T x)).$
MLE: $\hat{R}(w) = \sum_{i=1}^n log(1 + exp(-y_iw^Tx_i))$, is \green{convex}
and $\nabla_w \ell_w = \frac{1}{1+exp(-yw^Tx)} exp(-yw^Tx) (-yx)$, and
$exp(-yw^Tx) = 1$ if misclassified. Thus $\nabla_w \ell_w =
\frac{-yx}{1+exp(yw^Tx)}$. With L2 regularizer, take step in direction
$w\blue{(1-2\lambda\eta_t)} - \eta_t \nabla_w \ell_w(y, x)$. Nonlinear classification via
kernels: $\hat{\alpha} = \argmin_{\alpha} \sum_{i=1}^n \log (1+\exp(-y_i \alpha^T K_i)) + \lambda \alpha^T K \alpha$ and $\hat{P}(y | x, \hat{\alpha}) = \frac{1}{1+\exp(-y \sum_{j=1}^n \alpha_j k(x_j, x))}$ with $w = \sum_i \alpha_i x_i$. M-class: $P(Y = i | x,w_{[c]}) = \frac{\exp(w_i^T x)}{\sum_{j=1}^c \exp(w_j^T x)}$.

\green{Can obtain class probablities}, but \red{dense sols}.
