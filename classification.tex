\section{Classification}
$h(x) = \operatorname{sign}(w^T x)$

\textbf{Losses}: 0/1 loss: $\ell_{0/1}(w; x, y) = [y \neq  \operatorname{sign}(w^T x)]$

Perceptron loss: $\ell_p(w; x, y) = \max(0, -yw^Tx)$

Hinge loss: $\ell_H(w; x, y) = \max(0, 1-yw^Tx)$

$ \nabla_w \ell_p(w, x_i, y_i) = \begin{cases} 0,&\ \text{if $w^T x_i y_i \geq 0$ ($1$ if $\ell_H$)} \\ -y_i x_i&\ \text{else}\end{cases}$

\textbf{SGD}: GD requires sum over all data, slow for large datasets.

1. Choose random initial $w_0 \in \Rd$

2. For $t = 1, 2, \dots$ do:

(a) Choose $(x, y) \in D$ u.a.r (w/ replacement)

(b) Set $w_{t+1} = w_t - \eta_t \nabla \ell(w_t; x, y)$

SGD converges if $\sum_t \eta_t = \infty$ and $\sum_t \eta_t^2 < \infty$.

Mini-batch: Choose multiple datapoints at random; may converge
faster.

\textbf{Perceptron}: SGD with $\ell_p$ and $\eta = 1$.

$\hat{w} =\argmin_w \frac{1}{n} \sum_{i=1}^n \ell_p(w; x_i, y_i)$

If data linearly separable finds separator.

\textbf{SVM}: SGD with $\ell_H$ and regularization.

$\hat{w} =\argmin_w \frac{1}{n} \sum_{i=1}^n \ell_H(w; x_i, y_i) + \lambda ||w||_2^2$

$w_{t+1} = w_t (1 - 2 \eta_t \lambda) + \eta_t x_i y_i [y_i w_T x_i < 1]$ 

Often $\eta_t = \frac{1}{\lambda t}$. \green{Works on non-linearly separable data}, finds best
separator w.r.t.\ $\ell_H$.

\section{???orthogonal distance}: Let $ w \in \mathbb{R}^d $ and $H = \{ x \in \mathbb{R}^d | \langle w , z \rangle = 0 \} $be a hyperplane. The orthogonal distance of a point z $\in \mathbb{R}^d$ to $H$ can be computed as$ \frac{|\langle w, z \rangle |}{||w||}$. Specifically, if $w$ is a unit vector, the inner product $\langle w, z \rangle$ directly gives the distance of $z$ to $H$.