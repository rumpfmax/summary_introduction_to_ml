\documentclass[11pt,landscape,a4paper]{article}
\usepackage{amssymb,amsmath,amsthm,amsfonts}
\usepackage{multicol,multirow}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage[colorlinks=true,citecolor=blue,linkcolor=blue]{hyperref}
\usepackage{graphicx}

%page margins
\geometry{top=0.5cm,left=0.5cm,right=0.5cm,bottom=0.5cm}

%no header
\pagestyle{empty}


\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother
\setcounter{secnumdepth}{0}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

\begin{document}

\raggedright
\footnotesize

\begin{center}
     \Large{\textbf{Summary for Introduction to Machine Learning 2019}} \\
\end{center}
\begin{multicols}{4}
\setlength{\premulticols}{0pt}
\setlength{\postmulticols}{0pt}
\setlength{\multicolsep}{0pt}
\setlength{\columnsep}{0pt}
\section{General}
P-Norm:$||x||_p = (\Sigma_{i=1}^n |x_i|^p)^{\frac{1}{p}}$\\
Frobenious Norm:$||A||_F = \sqrt{\Sigma_{i,j} a_{ij}^2}$\\
Derivation rules: Chain rule: $D(f(g(x))) = Df(g(x))*Dg(x)$\\
positive definiteness: $A$ is p.s.d., then A is a real symmetric matrix and $x^TAx \geq 0$ for all x\\
Joint distribution: X, Y are RVs $F_{X,Y}(x,y) = \mathbb{P}(X \leq x, Y\leq y)$\\
Joint density:$ f_{X,Y}(x, y) = \frac{\delta^2 F}{\delta x \delta y}(x, y)$\\
Conditional Probability: $\mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B}{\mathbb{P}(B)}$\\
Law of total probability: $\mathbb{P}(B) = \Sigma_{i=1}^n \mathbb{P}(B|A_i)\mathbb{P}(A_i)$\\
Bayes rule: $\mathbb{P}(A|B) = \mathbb{P}(B|A)\frac{\mathbb{P}(A)}{\mathbb{P}(B)}$\\
Variance: $Var(X) = \mathbb{E}[(X - \mathbb{E}[X])^2] = \mathbb{E}[X^2] - \mathbb{E}[X]^2 \geq 0$\\

\section{Regression: Predict real valued labels}
\subsection{Linear Regression}
$f(x) = w_1 x_1 + \cdot \cdot \cdot + w_d x_d + w_0 = \widetilde{w}^T \widetilde{x}$ with \\
$\widetilde{w} = [w_1 \cdot \cdot \cdot w_d,\ w_0 ]$ and $\widetilde{x} = [x_1 \cdot \cdot \cdot x_d,\ 1]$ \\
Residual: $r_i = y_i - w^T x_i,\ x_i \in \mathbb{R}^d,\ y_i \in \mathbb{R}$ \\
Cost / Objective function (is convex): $\hat{R}(w) = \sum_{i=1}^n r_i^2 =  \sum_{i=1}^n (y_i - w^T x_i)^2$\\
Optimal weights: $w^* = \underset{w}{\operatorname{argmin}} \sum_{i=1}^n (y_i - w^T x_i)^2$\\
Closed form solution: $w^*=(X^T X)^{-1} X^T y$\\
Gradient: $\nabla_w \hat{R}(w) = [\frac{\delta}{\delta w_1} \hat{R}(w) \cdot \cdot \cdot \frac{\delta}{\delta w_d} \hat{R}(w)] =  -2 \sum_{i=1}^n r_i x_i^T$\\
Non-linear functions: $f(x) = \sum_{i=1}^D w_i \phi_i(x)$

\subsection*{Convex function}
$f : \mathbb{R}^d \rightarrow \mathbb{R} \text{ is convex} \Leftrightarrow x_1,x_2 \in \mathbb{R}^d, \lambda \in [0,1]:$ \\
$f(\lambda x_1 + (1-\lambda) x_2) \leq \lambda f(x_1) + (1-\lambda) f(x_2)$

\subsection{Gradient Descent}
1. Start at an arbitrary $w_0 \in \mathbb{R}^d$\\
2. For $t = 1,2,...$ do $w_{t+1} = w_t - \eta_t \nabla \hat{R}(w_t)$

\subsection*{Gaussian/Normal Distribution}
$f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} exp(-\frac{(x-\mu)^2}{2\sigma^2})$

\subsection*{Multivariate Gaussian}
$f(x) = \frac{1}{2\pi \sqrt{|\Sigma|}} e^{- \frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu)}$\\
$\Sigma =
\begin{pmatrix} 
\sigma_1^2 & \sigma_{12} \\
\sigma_{21} & \sigma_2^2
\end{pmatrix},\ 
\mu = 
\begin{pmatrix} 
\mu_1\\
\mu_2
\end{pmatrix}$

\subsection*{Empirical risk minimization}
Assumption: Data set generated iid from unknown distribution P: $(x_i , y_i) \sim P(X, Y).$\\ 
True risk: $R(w) = \int P(x,y) (y-w^Tx)^2 dx dy = \mathbb{E}_{x,y}[(y-w^Tx)^2]$\\
Empirical risk: $\hat{R}_D(w) = \frac{1}{|D|}\sum_{(x,y)\in D} (y-w^Tx)^2$\\
Generalization error: $|R(w) - \hat{R}_D(w)|$\\
Uniform convergence: $\operatorname{sup}_w\  |R(w) - \hat{R}_D(w)| \rightarrow 0 \text{ as } |D| \rightarrow 0$\\
In general, it holds that: $\mathop{\mathbb{E}}_D[\hat{R}_D(\hat{w}_D)] \leq \mathop{\mathbb{E}}_D[R(\hat{w}_D)]$, where $\hat{w}_D =  \operatorname{argmin}_w\ \hat{R}_D(w)$.

\subsection*{Cross-validation}
For each model $m$\\
For i = 1:k\\
1. Split data: $D = D_{train}^{(i)} \uplus D_{val}^{(i)}$\\
2. Train model: $\hat{w}_{i,m} = \operatorname{argmin}_w\ \hat{R}_{train}^{(i)}(w)$\\
3. Estimate error: $\hat{R}_m^{(i)} =  \hat{R}_{val}^{(i)}(\hat{w}_{i,m})$\\
After all iterations, select model:  $\hat{m} =  \operatorname{argmin}_w\ \frac{1}{k} \sum_{i=1}^k \hat{R}_m^{(i)}$

\subsection*{Ridge regression}
Regularization(corresponds to MAP estimation): $\operatorname{min}_w\ \frac{1}{n} \sum_{i=1}^n (y_i - w^Tx_i)^2 + \lambda ||w||_2^2 =  \operatorname{argmax}_w\ P(w)\Pi_i P(y_i | x_i w)$\\
Sparse regression (L1, convex) encourages coefficients to be exactly 0 - automatic feature selection\\
Closed form solution: $\hat{w} =(X^T X + \lambda I)^{-1} X^T y$\\
Gradient: $\nabla_w (\frac{1}{n} \sum_{i=1}^n (y_i - w^Tx_i)^2 + \lambda ||w||_2^2) = \nabla_w \hat{R}(w) + 2 \lambda w$

\subsection*{Standardization}
Goal: each feature: $\mu = 0$, $\sigma^2 = 1$: $\tilde{x}_{i,j} = \frac{(x_{i,j}-\hat{\mu}_j)}{\hat{\sigma}_j}$\\
$\hat{\mu}_j = \frac{1}{n}\sum_{i=1}^n x_{i,j}$, $\hat{\sigma}_j^2 = \frac{1}{n}\sum_{i=1}^n {(x_{i,j}-\hat{\mu}_j)}^2$ 

\section*{Classification}
$h(x) = \operatorname{sign}(w^T x)$\\
\subsection{Losses}
0/1 loss: $\ell_{0/1}(w; x, y) = [y \neq  \operatorname{sign}(w^T x)]$ \\
Perceptron loss: $\ell_p(w; x, y) = \max(0, -yw^Tx)$ \\
Hinge loss: $\ell_H(w; x, y) = \max(0, 1-yw^Tx)$\\
$\hat{w} = \operatorname{argmin}_w \frac{1}{n} \sum_{i=1}^n \ell_p(w; x_i, y_i)$\\
$ \nabla_w \ell_p(w, x_i, y_i) = \begin{cases} 0,&\ \text{if $w^T x_i y_i \geq 0$ ($1$ if $\ell_H$)} \\ -y_i x_i&\ \text{else}\end{cases}$\\

\subsection{SGD}
GD requires sum over all data, slow for large datasets.\\
1. Choose random initial $w_0 \in \mathbb{R}^d$\\
2. For $k = 0, 1, \dots$:\\
(a) Choose $(x, y) \in D$ u.a.r (w/ replacement)\\
(b) Set $w_{t+1} = w_t - \eta_t \nabla \ell(w_t; x, y)$\\
SGD converges if $\sum_t \eta_t = \infty$ and $\sum_t \eta_t^2 < \infty$.\\
Mini-batch: Choose multiple datapoints at random; may converge
faster.\\

\subsection{Perceptron}
SGD with $\ell_p$ and $\eta = 1$. If data linearly separable finds separator.


\section{Dimension Reduction in unsupervised learning}
\subsection*{Principal Component Analysis (linear)}
Given $ D \subseteq \mathbb{R}^d, 1 \leq k \leq d, \Sigma = \frac{1}{n}\sum_{i=1}^n x_i x_i ^T, \mu = \frac{1}{n}\sum_i x_i = 0 $ (data is centered)\\
$(W, z_1,...,z_n) = \operatorname{argmin} \sum_{i=1}^n ||W z_i - x_i||_2^2 $\\where $ W \in \mathbb{R}^{d \times k}$ is orthogonal ,$ z_1,...z_n \in \mathbb{R}^k $ is given by $ W = (v_1|...|v_k) $and$ z_i = W^T x_i = f(x)$ where $ \Sigma = \Sigma_{i=1}^d \lambda_i v_i v_i^T$ where $\lambda_1 \geq ... \geq \lambda_d \geq 0$\\
The projection is chosen to minimize the reconstruction error, choose k such that most of the variance is explained (like k-means)
\subsection*{Kernel PCA (nonlinear)}
For k = 1: Kernel PCA\\
$\alpha^* = \underset{\alpha^T K \alpha = 1}{\operatorname{argmax}} \alpha^T K^T K \alpha$\\
With $K = \Sigma_{i=1}^n \lambda_i v_i v_i^T$ $ (\lambda_1 \geq ... \geq \lambda_d \geq 0)$ $ \alpha* = \frac{1}{\sqrt{\lambda_1}} v_1$\\
For general k: Kernel PCA\\
The kernel principal components are given by $\alpha^{(1)},...,\alpha^{(k)} \in \mathbb{R}^n$\\
$\alpha^{(i)} = \frac{1}{\sqrt{\lambda_i}}$ with $K = \Sigma_{i=1}^n \lambda_i v_i v_i^T$\\
A new point x is projected as z, $z_i = \Sigma_{j=1}^n \alpha_j^{(i)} k(x, x_j)$\\
Kernel-PCA corresponds to applying PCA in the feature space induced by the kernel k.\\
centering a kernel: $K' = K - KE - EK + EKE$ where $E = \frac{1}{n}[1,...,1][1,...1]^T$\\
- complexity grow with number of data points, requires data specified as kernel
\subsection*{Autoencoders}
Goal: learn identity function $x \approx f(x; \theta)$\\
$f(x; \theta) = f_{dec}(f_{enc}(x; \theta_1);\theta_2)$\\
NN autoencoders are ANNs where one output unit for each of d input units, nr of hidden units smaller than nr of inputs. Optimize w s.t. output agrees with input.\\
If activation func. is the identity, fitting NN autoencoder is equivalent to PCA.
\section{Decision Theory}
\subsection*{Bayesian Decision Theory}
Given: $P(y|x)$, set of actions $A$ and cost function $C: Y \times A \rightarrow \mathbb{R}$\\
$a^* = \underset{a \in A}{\operatorname{argmin}} \mathbb{E}_y [C(y, a) | x]$ (cost for prediction a when true label is y) \\
for logistic regression:$ \underset{y}{\operatorname{argmax}} P(y|x) = sign (w^Tx)$ (most likely class)\\
Doubtful logistic regression is when we pick the most likely class only if we are confident enough.\\
\subsection{MAP}
1. choose likelihood function $\rightarrow$ loss function\\
2. choose prior $\rightarrow$ regularizer\\
3. optimize for MAP parameters, choose hyperparameters through cross-validation\\
4. make predictions via Bayesian Decision Theory
\end{multicols}

\end{document}
