\section{Generalized Mixture Models}
Labels maybe unknown, want to cluster data.

Model $P(x, \theta)$ as \emph{conv. comb.} of Gauss. distr.:
$\sum_{i=1}^c w_i \mathcal{N}(x; \mu_i, \Sigma_i)$ with $w_i > 0$ and $\sum_i w_i =
1$. $(\mu^*, \Sigma^*, w^*) = \argmin - \sum_{i=1}^n \log \sum_{j=1}^k w_j \mathcal{N}(x_i; \mu_j,
\Sigma_j)$. Constr. ($\Sigma$ p.s.d.) \red{hard to maint.} in SGD. Fitting GMM = Train a GBC without labels.

\textbf{Hard-EM}: E-step: Pred. most likely class for all $x_i$: $z_i^{(t)} = \argmax_z P(z \mid
x_i, \theta^{(t-1)}) = \argmax_z P(z | \theta^{(t-1)}) P(x_i | z, \theta^{(t-1)}) $ with $\theta^{(t)} = [w_{1:c}^{(t)}, \mu_{1:c}^{(t)}, \Sigma_{1:c}^{(t)}]$. M-step: Comp. $\theta^{(t)}$ as MLE as for GBC:
$\theta^{(t)} = \argmax_\theta P(D^{(t)} \mid \theta)$.

\red{Too much info} extr. from each label, \red{overlapping} clusters
not detected, \red{fixed label} if model uncertain. k-Means Algo is equiv. to Hard-EM if $w_j = 1/k$ and $\Sigma_j = I \cdot \sigma^2$.

\textbf{Soft-EM}: $\gamma_j(x)$ is prob. $x$ in clust. $j$: $\gamma_j(x) = P(Z = j \mid x, \Sigma, \mu, w) = \frac{w_j P(x \mid \Sigma_j, \mu_j)}{\sum_\ell w_\ell P(x \mid \Sigma_\ell, \mu_\ell)}$

E-Step: Calculate prob $\gamma_j(x_i)$ for all $i, j$ based on $\mu^{(t-1)}$,
$\Sigma^{(t-1)}$ and $w^{(t-1)}$.

M-Step: Adjust parameters: $w_j^{(t)} = \frac{1}{n} \sum_{i=1}^n \gamma_j^{(t)}(x_i)$, $\mu_j^{(t)} = \frac{\sum_{i=1}^n \gamma_j^{(t)}(x_i) x_i}{\sum_{i=1}^n  \gamma_j^{(t)}(x_i)}$ and
$\Sigma_j^{(t)} = \frac{\sum_{i=1}^n  \gamma_j^{(t)}(x_i) (x_i - \mu^{(t)}_j) (x_i -
\mu_j^{(t)})^T}{\sum_{i=1}^n \gamma_j^{(t)}(x_i)}$.

\red{Init. sensitive}, \red{nonconvex objective}. Init. $w$ with unif. distr., $\mu$ by k-means++ and $\Sigma$ as spherical (acc. to emp. var. in data). Choose $k$ \green{via CV}. Avoid \red{degeneracy} by \green{add. term $\nu^2 I$} to $\Sigma_j^{(t)}
\rightarrow$ Wishart-prior.

\textbf{Semi-supervised learning}: For points with label $y_i$: $\gamma_j^{(t)}(x_i) = [j = y_i]$

\textbf{GANs}: $\min_{w_G} \max_{w_D} \mathbb{E}_{x \sim \text{Data}} \log D(x; w_D) + \mathbb{E}_{x \sim \mathcal{N}} \log(1 - D(G(z; w_G); w_D))$, \red{Mode Collapse}: gen. prod. limit. variety of samples, \red{Data memorization}, Simult. training $\rightarrow$ \red{oscillations}, \red{Can't comp. likelih. on holdout set}.
