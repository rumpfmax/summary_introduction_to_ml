\section{Regression: Predict real valued labels}
\textbf{Linear Regression}: Goal: Measure distance between predicted and target values
$f(x) = w_1 x_1 + \cdot \cdot \cdot + w_d x_d + w_0 = \widetilde{w}^T \widetilde{x}$ with
$\widetilde{w} = [w_1 \cdot \cdot \cdot w_d,\ w_0 ]$ and $\widetilde{x} = [x_1 \cdot \cdot \cdot x_d,\ 1]$

Residual: $r_i = y_i - w^T x_i,\ x_i \in \mathbb{R}^d,\ y_i \in \mathbb{R}$

Cost / Objective function (is convex): $\hat{R}(w) = \sum_{i=1}^n r_i^2 =  \sum_{i=1}^n (y_i - w^T x_i)^2$

Optimal weights: $w^* = \underset{w}{\operatorname{argmin}} \sum_{i=1}^n (y_i - w^T x_i)^2$

Closed form solution: $w^*=(X^T X)^{-1} X^T y$

Gradient: $\nabla_w \hat{R}(w) = [\frac{\delta}{\delta w_1} \hat{R}(w) \cdot \cdot \cdot \frac{\delta}{\delta w_d} \hat{R}(w)] =  -2 \sum_{i=1}^n r_i x_i^T$

Non-linear functions: $f(x) = \sum_{i=1}^D w_i \phi_i(x)$

\textbf{Fisher consistency}: Given a surrogate loss function $\psi : Y \times S \rightarrow \mathbb{R}$, the surrogate is said to be consistent with respect to the loss $L : Y \times S \rightarrow \mathbb{R}$, if every minimizer f of the surrogate risk function $R_{\psi}(f)$ is also a minimizer of the risk function $R_L (f)$. E.g. the hinge and the logistic losses are consistent with respect to the 0-1 loss.

\textbf{Classification losses}: $L_{perceptron}: \{ -1, 1 \} \times \mathbb{R} \rightarrow \mathbb{R}: y, f(x) \rightarrow \operatorname{max} (0, -yf(x))$

Find the best separation hyperplane

$L_{hinge}: \{ -1, 1 \} \times \mathbb{R} \rightarrow \mathbb{R}: y, f(x) \rightarrow \operatorname{max} (0, 1-yf(x))$

Find large separation margin

$L_{perceptron}: \{ -1, 1 \} \times \mathbb{R} \rightarrow \mathbb{R}: y, f(x) \rightarrow \operatorname{max} log(1 + exp(-yf(x))$

Link to cross entropy and probabilistic interpretation

\textbf{Classification}: Accuracy: $\frac{TP + TN}{TP + TN + FP + FN}$

Recall/Sensitivity/True positive rate/TPR: $\frac{TP}{TP + FN}$

Specify or True negative rate/TNR: $\frac{TN}{TN + FP}$

F1 score: $2* \frac{Precision \ast Recall}{Precision + Recall}$

\textbf{Convex function}: $f : \mathbb{R}^d \rightarrow \mathbb{R} \text{ is convex} \Leftrightarrow x_1,x_2 \in \mathbb{R}^d, \lambda \in [0,1]:$

$f(\lambda x_1 + (1-\lambda) x_2) \leq \lambda f(x_1) + (1-\lambda) f(x_2)$

\textbf{Gradient Descent}: 1. Start at an arbitrary $w_0 \in \mathbb{R}^d$

2. For $t = 1,2,...$ do $w_{t+1} = w_t - \eta_t \nabla \hat{R}(w_t)$

\textbf{Gaussian/Normal Distribution}:
$f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} exp(-\frac{(x-\mu)^2}{2\sigma^2})$

\textbf{Multivariate Gaussian}: $f(x) = \frac{1}{2\pi \sqrt{|\Sigma|}} e^{- \frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu)}$

$\Sigma =
\begin{pmatrix} 
\sigma_1^2 & \sigma_{12} \\
\sigma_{21} & \sigma_2^2
\end{pmatrix},\ 
\mu = 
\begin{pmatrix} 
\mu_1\\
\mu_2
\end{pmatrix}$

\textbf{Empirical risk minimization}: Assumption: Data set generated iid from unknown distribution P: $(x_i , y_i) \sim P(X, Y).$

True risk: $R(w) = \int P(x,y) (y-w^Tx)^2 dx dy = \mathbb{E}_{x,y}[(y-w^Tx)^2]$

Empirical risk: $\hat{R}_D(w) = \frac{1}{|D|}\sum_{(x,y)\in D} (y-w^Tx)^2$

Generalization error: $|R(w) - \hat{R}_D(w)|$

Uniform convergence: $\operatorname{sup}_w\  |R(w) - \hat{R}_D(w)| \rightarrow 0 \text{ as } |D| \rightarrow 0$

In general, it holds that: $\mathop{\mathbb{E}}_D[\hat{R}_D(\hat{w}_D)] \leq \mathop{\mathbb{E}}_D[R(\hat{w}_D)]$, where $\hat{w}_D =  \operatorname{argmin}_w\ \hat{R}_D(w)$.

\textbf{Cross-validation}: For each model $m$

For i = 1:k

1. Split data: $D = D_{train}^{(i)} \uplus D_{val}^{(i)}$

2. Train model: $\hat{w}_{i,m} = \operatorname{argmin}_w\ \hat{R}_{train}^{(i)}(w)$

3. Estimate error: $\hat{R}_m^{(i)} =  \hat{R}_{val}^{(i)}(\hat{w}_{i,m})$

After all iterations, select model:  $\hat{m} =  \operatorname{argmin}_w\ \frac{1}{k} \sum_{i=1}^k \hat{R}_m^{(i)}$

\textbf{Ridge regression}: Regularization(corresponds to MAP estimation): $\operatorname{min}_w\ \frac{1}{n} \sum_{i=1}^n (y_i - w^Tx_i)^2 + \lambda ||w||_2^2 =  \operatorname{argmax}_w\ P(w)\Pi_i P(y_i | x_i w)$

Sparse regression (L1, convex) encourages coefficients to be exactly 0 - automatic feature selection

Closed form solution: $\hat{w} =(X^T X + \lambda I)^{-1} X^T y$

Gradient: $\nabla_w (\frac{1}{n} \sum_{i=1}^n (y_i - w^Tx_i)^2 + \lambda ||w||_2^2) = \nabla_w \hat{R}(w) + 2 \lambda w$

\textbf{Standardization}: Goal: each feature: $\mu = 0$, $\sigma^2 = 1$: $\tilde{x}_{i,j} = \frac{(x_{i,j}-\hat{\mu}_j)}{\hat{\sigma}_j}$

$\hat{\mu}_j = \frac{1}{n}\sum_{i=1}^n x_{i,j}$, $\hat{\sigma}_j^2 = \frac{1}{n}\sum_{i=1}^n {(x_{i,j}-\hat{\mu}_j)}^2$