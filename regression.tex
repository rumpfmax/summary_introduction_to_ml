\section{Regression}
\textbf{Linear Regression}: Goal: Measure distance between predicted and target values
$f(x) = w_1 x_1 + \cdot \cdot \cdot + w_d x_d + w_0 = \widetilde{w}^T \widetilde{x}$ with
$\widetilde{w} = [w_1 \cdot \cdot \cdot w_d,\ w_0 ]$ and $\widetilde{x} = [x_1 \cdot \cdot \cdot x_d,\ 1]$.

Residual: $r_i = y_i - w^T x_i,\ x_i \in \mathbb{R}^d,\ y_i \in \mathbb{R}$.

Objective function (convex): $\hat{R}(w) = \sum_{i=1}^n r_i^2$.

$w^* = \argmin_w \sum_{i=1}^n (y_i - w^T x_i)^2$.

Closed form solution: $w^*=(X^T X)^{-1} X^T y$.

Gradient: $\nabla_w \hat{R}(w) =  -2 \sum_{i=1}^n r_i x_i^T$.

Non-linear functions: $f(x) = \sum_{i=1}^D w_i \phi_i(x)$.

\textbf{Gradient Descent}: 1. Start $w_0 \in \mathbb{R}^d$.

2. For $t = 1,2,...$ do $w_{t+1} = w_t - \eta_t \nabla \hat{R}(w_t)$.

\textbf{Empirical risk minimization}: Assumption: Data set generated iid from unknown distribution $P$: $(x_i , y_i) \sim P(X, Y).$

True risk: $R(w) = \int P(x,y) (y-w^Tx)^2 dx dy = \mathbb{E}_{x,y}[(y-w^Tx)^2]$

Emp. risk: $\hat{R}_D(w) = \frac{1}{|D|}\sum_{(x,y)\in D} (y-w^Tx)^2$

Generalization error: $|R(w) - \hat{R}_D(w)|$

Uniform convergence: $\operatorname{sup}_w\  |R(w) - \hat{R}_D(w)| \rightarrow 0 \text{ as } |D| \rightarrow 0$

In general, it holds that: $\mathop{\mathbb{E}}_D[\hat{R}_D(\hat{w}_D)] \leq \mathop{\mathbb{E}}_D[R(\hat{w}_D)]$, where $\hat{w}_D =  \operatorname{argmin}_w\ \hat{R}_D(w)$.

\textbf{Cross-validation}: For each model $m$ and for $i = 1:k$:

1. Split data: $D = D_{train}^{(i)} \uplus D_{val}^{(i)}$.

2. Train model: $\hat{w}_{i,m} = \argmin_w\ \hat{R}_{train}^{(i)}(w)$.

3. Estimate error: $\hat{R}_m^{(i)} =  \hat{R}_{val}^{(i)}(\hat{w}_{i,m})$.

Select model:  $\hat{m} =  \operatorname{argmin}_m\ \frac{1}{k} \sum_{i=1}^k \hat{R}_m^{(i)}$.

\textbf{Ridge regression}: Regularization (corresponds to MAP estimation): 

$\min_w\ \frac{1}{n} \sum_{i=1}^n (y_i - w^Tx_i)^2 + \lambda ||w||_2^2$.

Closed form: $\hat{w} =(X^T X + \lambda I)^{-1} X^T y$

Gradient: $-2 \sum_{i=1}^n r_i x_i^T + 2 \lambda w$

\textbf{Standardization}: Goal: each feature: $\mu = 0$, $\sigma^2 = 1$: $\tilde{x}_{i,j} = \frac{(x_{i,j}-\hat{\mu}_j)}{\hat{\sigma}_j}$ with 

$\hat{\mu}_j = \frac{1}{n}\sum_{i=1}^n x_{i,j}$, $\hat{\sigma}_j^2 = \frac{1}{n}\sum_{i=1}^n {(x_{i,j}-\hat{\mu}_j)}^2$.