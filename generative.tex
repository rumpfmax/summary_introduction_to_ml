\section{Generative Modeling}

Est. \emph{joint distribution} $P(X,Y)$ instead of $P(Y \mid X)$. Pred. $y = \sign(f(x))$ Cond.\
distr.\ derived from joint:  1. Est. $P(Y)$, 2. Est. $P(x|y) \forall y$, 3. Use Bayesâ€˜ rule: $P(y|x) = P(y) P(x|y)/P(x)$ where $P(x) = \sum_{y'} P(x, y')$.  For $c=2$ \emph{discriminant function} $f(x)
= \log\frac{P(Y = 1 \mid x)}{P(Y = -1 \mid x)}$, which is $+1$ if $P(Y = 1 \mid x) > 0.5$.

\textbf{Naive Bayes}: Model $y$ as categorical, and features \red{conditionally
independent} given $y$, e.g.\ Gaussian NB, assumes $P(x_i | y) =
\mathcal{N}(x_i | \mu_{y,i}, \sigma^2_{y,i})$. Is linear class., equiv. to log. reg. if assumpt. are met. $f(x) = w^T x + w_0$
where $w_0 = \log\frac{p_+}{1 - p_+} + \sum_{i=1}^d \frac{\mu_{-,i}^2 -
\mu_{+,i}^2}{2 \sigma_i^2}$ and $w_i = \frac{\mu_{+,i} - \mu{-,i}}{\sigma_i^2}$. Cond. indep. assumpt. can make pred. \red{overconfident}. \green{\# parameters = $O(c d)$}, Compl. (mem. + inference) \green{lin. in d}.

\textbf{Categorical Naive Bayes}: $P(X_i =c|Y =y)=\theta^{(i)}_{c|y}$. MLE prior: $\hat{p}_y = \frac{\# y}{n}$, MLE feat.\ distr.: $\theta^{(i)}_{c|y} = \frac{\text{Count}(X_i = c, Y = y)}{\# y}$. \red{Exponentially (in d) parameters}, \red{Way to overfit}.

\textbf{Gaussian Bayes}: $P(x|y) = \mathcal{N}(x; \mu_y, \Sigma_y)$. MLE prior: $\hat{p}_y = \frac{\# y}{n}$, MLE feat.\ distr.: $\hat{\mu}_k = \frac{1}{\# y} \sum_{i: y_i = y} x_i$ and $\hat{\Sigma}_y = \frac{1}{\# y}
\sum_{i: y_i = y} (x_i - \hat{\mu}_y) (x_i - \hat{\mu}_y)^T$. $f(x) = \log\frac{p}{1-p} + \frac{1}{2} [\log\frac{|\hat{\Sigma}_-|}{|\hat{\Sigma}_+|} + ( (x - \hat{\mu}_-)^T \hat{\Sigma}_-^{-1} (x-\hat{\mu}_-) ) - ( (x - \hat{\mu}_+)^T \hat{\Sigma}_+^{-1} (x-\hat{\mu}_+) ) ]$. \green{Capt. corr. of feat.}, \green{no overconf.}, \red{\# param. = $O(c d^2)$}, compl. \red{ quadr. in d}.

\textbf{Fischer's LDA}: Assume $p = 0.5$ and $\hat{\Sigma}- = \hat{\Sigma}+ = \hat{\Sigma}$. Then $f(x) =
x^T \hat{\Sigma}^{-1} (\hat{\mu}_+ - \hat{\mu}_-)+ \frac{1}{2} \left( \hat{\mu}_-^T \hat{\Sigma}^{-1} \hat{\mu}_- -
\hat{\mu}_+^T \hat{\Sigma}^{-1} \hat{\mu}_+\right)$.

Equiv. to log. reg. if assump. are met. LDA can be viewed as proj. to a 1-dim. subsp. that maxes ratio of between-class and within-class var.. PCA (k=1) maxes the var. of the res. 1-dim. proj.. Gen. model, used to \green{detect outliers}, not rob. against viol. of \red{normality of X assumpt.}.

\textbf{Conjugate distr.}: Posterior is same family as prior. Ex.: $\Beta(\theta,
\alpha_+, \alpha_-)$ and $\Beta(\theta, n_+ + \alpha_+, n_- + \alpha_-)$.
